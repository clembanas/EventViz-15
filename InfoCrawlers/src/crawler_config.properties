# Debug settings
# Debug database
debug.database=false
# Debug database updates
debug.database.updates=false
# Debug database query results
debug.database.results=false
# Debug remote object manager
debug.remote_obj_mgr=false
# Debug remote object manager's TCP connections
debug.remote_obj_mgr.connection=false
# Debug remote object manager's remote object actions
debug.remote_obj_mgr.remote_object=false
# Debug the arguments used for a remote object's method invocation 
debug.remote_obj_mgr.method_args=false
# Debug SPARQL
debug.sparql=false
# Debug SPARQL queries
debug.sparql.queries=false
# Debug SPARQL query results
debug.sparql.results=false
# Don't output query strings when debugging SPARQL queries
debug.sparql.no_query_string=true
# Don't output the prefixes used for SPARQL queries
debug.sparql.no_query_prefix=true
# Debug crawler manager
debug.crawler.manger=true
# Debug crawler base class
debug.crawler.base=true
# Debug database based crawler class
debug.crawler.db_query_based=true
# Debug SPARQL based crawler class
debug.crawler.sparql_based=false
# Debug Eventful crawler
debug.crawler.eventful=true
# Debug City-Info crawler
debug.crawler.city_infos=true
# Debug Band-Info crawler
debug.crawler.band_infos=true
# Maximum count of database logs
debug.max_db_logs=10000
# File used to output debug information in addition to database logs (an empty string deactivates 
# file output)
debug.log_file=
# File used to output exception information in addition to database logs (an empty string 
# deactivates file output)
debug.exception_log_file=
# Thread monitoring interval in msec (0 deactivates monitoring)
debug.thread_monitor.interval=1000
# Time in sec after a warning is output in case of no thread progress (0 deactivates warnings)
debug.thread_monitor.warn_no_thd_progress=300
# Time interval in sec for outputting thread liveness info (0 deactivates liveness info)
debug.thread_monitor.thd_liveness_info=0
# Time interval in sec for outputting runtime info (0 deactivates runtime info)
debug.thread_monitor.runtime_info=600

# Database connector-class to be used
database.connector_class=DBConnector_MySQL

# Remote object manager port
remote_obj_mgr.port=9125

# Crawler master host (needs only to be set on crawler slaves)
crawler.master.host=
# Host which performs the job controlling of all crawlers (if not specified the head node host is
# used)
#crawler.job_controller.host=

# Time point when the crawlers have to be executed (every day)
crawler.crawl_time=23:30

# Eventful crawler settings
# Maximum days from today to crawl 
crawler.eventful.max_days=30
# Maximum size of each event page
crawler.eventful.page_size=100
# Maximum count of event pages
crawler.eventful.max_pages=1000
# Maximum count worker threads
crawler.eventful.worker_thd_count=5
# Eventful.com access settings
crawler.eventful.uname=RobertBierbauer
crawler.eventful.pword=3v3ntful
crawler.eventful.apikey=T79WLsPLQkRDJZxv
crawler.eventful.category=music
# Connection timeout in msec for Eventful.com
crawler.eventful.connection_timeout=120000
# Read timeout in msec for Eventful.com
 crawler.eventful.read_timeout=120000
# Comma separated IP addresses of the hosts to be used for the crawler-slaves. An empty value causes 
# the crawler to only run on the master node
crawler.eventful.hosts=

# Sparql based crawler settings
# DBpedia endpoints to be used 
crawler.sparql_based.dbpedia_endpoint1=http://dbpedia.org/sparql
crawler.sparql_based.dbpedia_endpoint2=http://live.de.dbpedia.org/sparql
# Maximum count of SPARQL query retries
crawler.sparql_based.max_query_retries=30
# Query retry delay in milliseconds
crawler.sparql_based.query_retry_delay=500
# SPARQL connection timeout in msec
crawler.sparql_based.connection_timeout=120000
# SPAQRL read timeout in msec
crawler.sparql_based.read_timeout=120000
# Default SPARQL query limit (may be overwritten by some queries)
crawler.sparql_based.def_query_limit=100
# Maximum cache size in Bytes
crawler.sparql_based.max_cache_size=500000

# City infos crawler settings
# Update interval of city information in hours
crawler.city_info.update_interval=420
# Maximum page size
crawler.city_info.page_size=100
# Maximum worker count
crawler.city_info.worker_thd_count=5
# Comma separated IP addresses of the hosts to be used for the crawler-slaves. An empty value causes 
# the crawler to only run on the master node
crawler.city_info.hosts=

# City infos crawler settings
# Update interval of band information in hours
crawler.band_info.update_interval=420
# Maximum page size
crawler.band_info.page_size=100
# Maximum worker count
crawler.band_info.worker_thd_count=5
# Comma separated IP addresses of the hosts to be used for the crawler-slaves. An empty value causes 
# the crawler to only run on the master node
crawler.band_info.hosts=
